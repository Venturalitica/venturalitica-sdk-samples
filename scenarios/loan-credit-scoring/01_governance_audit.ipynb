{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3020553f",
   "metadata": {},
   "source": [
    "# üéØ Level 1: ML Training with Governance\n",
    "\n",
    "**‚è±Ô∏è Time:** 8 minutes  \n",
    "**üéì Complexity:** ‚≠ê‚≠ê Intermediate  \n",
    "**üéØ Goal:** Integrate SDK into a complete ML training pipeline\n",
    "\n",
    "## What you'll learn:\n",
    "1. **Pre-training audits** - Validate data quality before training\n",
    "2. **Governance wrapping** - Wrap your sklearn models with compliance checks\n",
    "3. **Post-training audits** - Automatic fairness validation on predictions\n",
    "4. **Actionable compliance reports** - Real OSCAL-native policy enforcement\n",
    "\n",
    "---\n",
    "\n",
    "### üéì The ML Governance Lifecycle\n",
    "\n",
    "Let's build a credit scoring model that is both **accurate** and **compliant**! üöÄ\n",
    "\n",
    "Most ML projects focus on **accuracy** but ignore **compliance** until production‚Äîwhen it's expensive to fix. This notebook shows the **governed ML lifecycle**:\n",
    "\n",
    "- **Trust**: Auditable governance helps you explain decisions to regulators, customers, and auditors\n",
    "\n",
    "```- **Cost savings**: Catching bias in data (pre-training) is 10x cheaper than retraining\n",
    "\n",
    "[Data] ‚Üí [Pre-Audit] ‚Üí [Train] ‚Üí [Post-Audit] ‚Üí [Deploy]- **Regulatory pressure**: EU AI Act, US Fair Lending laws require documented fairness checks\n",
    "\n",
    "   ‚Üì          ‚Üì           ‚Üì           ‚Üì            ‚Üì**Why this matters:**\n",
    "\n",
    "  Raw      Quality    Wrapped    Fairness    Monitoring\n",
    "\n",
    "         checks      model      validation   in prod```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6439aae6",
   "metadata": {},
   "source": [
    "### How to use this notebook\n",
    "- **Goal:** Train a credit model and keep it compliant at each step.\n",
    "- **Data:** Full German Credit dataset (1,000 loans). Runs even if the local CSV is missing by using SDK sample loaders.\n",
    "- **Policies:** `policies/loan/risks.oscal.yaml` (data quality) and `policies/loan/governance-baseline.oscal.yaml` (fairness during inference).\n",
    "- **Flow:** Load ‚Üí Pre-audit ‚Üí Train (wrapped) ‚Üí Post-audit ‚Üí Interpret results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e287f82d",
   "metadata": {},
   "source": [
    "## üì¶ Setup: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00ca7553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import venturalitica as vl\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pathlib import Path\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2754d5",
   "metadata": {},
   "source": [
    "## Step 1: Load & Prepare Data üìä\n",
    "\n",
    "We'll load the **German Credit Data** (1,000 real loan applications from UCI Repository Dataset #144).\n",
    "\n",
    "- `foreign_worker` (nationality-based discrimination)\n",
    "\n",
    "### üéì About the German Credit Dataset- `age` (age-based discrimination is prohibited in many jurisdictions)\n",
    "\n",
    "- `gender` (male/female)\n",
    "\n",
    "**Origin**: Collected by Prof. Hans Hofmann (University of Hamburg, 1994)  **Protected attributes** (where discrimination is illegal):\n",
    "\n",
    "**Use case**: Predict creditworthiness based on 20 attributes (age, job, credit history, etc.)  \n",
    "\n",
    "**Why it's important**: One of the first datasets where researchers documented gender bias in lending decisions- **Target**: `class` = \"good\" or \"bad\" credit risk (we convert to binary 1/0)\n",
    "\n",
    "- **Categorical**: `checking_status`, `credit_history`, `purpose`, `gender`\n",
    "\n",
    "**Key attributes**:- **Numerical**: `age`, `credit_amount`, `duration` (loan term in months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2fd43c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading German Credit Data...\n",
      "\n",
      "‚úÖ Loaded 1000 loan applications\n",
      "   Features: 24 columns\n",
      "   Target distribution: {1: 700, 0: 300}\n",
      "\n",
      "üìã First 3 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>checking_status</th>\n",
       "      <th>duration</th>\n",
       "      <th>credit_history</th>\n",
       "      <th>purpose</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>savings_status</th>\n",
       "      <th>employment</th>\n",
       "      <th>installment_commitment</th>\n",
       "      <th>personal_status_sex</th>\n",
       "      <th>other_parties</th>\n",
       "      <th>...</th>\n",
       "      <th>housing</th>\n",
       "      <th>existing_credits</th>\n",
       "      <th>job</th>\n",
       "      <th>num_dependents</th>\n",
       "      <th>own_telephone</th>\n",
       "      <th>foreign_worker</th>\n",
       "      <th>class</th>\n",
       "      <th>target</th>\n",
       "      <th>gender</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11</td>\n",
       "      <td>6</td>\n",
       "      <td>A34</td>\n",
       "      <td>A43</td>\n",
       "      <td>1169</td>\n",
       "      <td>A65</td>\n",
       "      <td>A75</td>\n",
       "      <td>4</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A152</td>\n",
       "      <td>2</td>\n",
       "      <td>A173</td>\n",
       "      <td>1</td>\n",
       "      <td>A192</td>\n",
       "      <td>A201</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>Senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A12</td>\n",
       "      <td>48</td>\n",
       "      <td>A32</td>\n",
       "      <td>A43</td>\n",
       "      <td>5951</td>\n",
       "      <td>A61</td>\n",
       "      <td>A73</td>\n",
       "      <td>2</td>\n",
       "      <td>A92</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A152</td>\n",
       "      <td>1</td>\n",
       "      <td>A173</td>\n",
       "      <td>1</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>female</td>\n",
       "      <td>Young</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A14</td>\n",
       "      <td>12</td>\n",
       "      <td>A34</td>\n",
       "      <td>A46</td>\n",
       "      <td>2096</td>\n",
       "      <td>A61</td>\n",
       "      <td>A74</td>\n",
       "      <td>2</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A152</td>\n",
       "      <td>1</td>\n",
       "      <td>A172</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>Senior</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  checking_status  duration credit_history purpose  credit_amount  \\\n",
       "0             A11         6            A34     A43           1169   \n",
       "1             A12        48            A32     A43           5951   \n",
       "2             A14        12            A34     A46           2096   \n",
       "\n",
       "  savings_status employment  installment_commitment personal_status_sex  \\\n",
       "0            A65        A75                       4                 A93   \n",
       "1            A61        A73                       2                 A92   \n",
       "2            A61        A74                       2                 A93   \n",
       "\n",
       "  other_parties  ...  housing existing_credits   job num_dependents  \\\n",
       "0          A101  ...     A152                2  A173              1   \n",
       "1          A101  ...     A152                1  A173              1   \n",
       "2          A101  ...     A152                1  A172              2   \n",
       "\n",
       "  own_telephone  foreign_worker class  target  gender age_group  \n",
       "0          A192            A201     1       1    male    Senior  \n",
       "1          A191            A201     2       0  female     Young  \n",
       "2          A191            A201     1       1    male    Senior  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"üìä Loading German Credit Data...\\n\")\n",
    "\n",
    "dataset_path = Path(\"../../datasets/loan/german_credit.csv\")\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Prepare required columns\n",
    "df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "df = df.dropna(subset=['age'])  # Drop rows with invalid age\n",
    "df['target'] = pd.to_numeric(df['target'], errors='coerce').astype('int64')\n",
    "df['age_group'] = pd.cut(df['age'], bins=[0, 25, 45, 100], labels=['Young', 'Adult', 'Senior'])\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df)} loan applications\")\n",
    "print(f\"   Features: {df.shape[1]} columns\")\n",
    "print(f\"   Target distribution: {df['target'].value_counts().to_dict()}\")\n",
    "print(f\"\\nüìã First 3 rows:\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a104ecf4",
   "metadata": {},
   "source": [
    "**Why these prep steps?**\n",
    "- `age` ‚Üí numeric and non-null to support age binning in policies.\n",
    "- `target` ‚Üí int for binary classification (1=good, 0=bad).\n",
    "- `gender` ‚Üí required for fairness audits; fallback prevents crashes.\n",
    "- `age_group` ‚Üí categorical bin used by policy controls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819730d1",
   "metadata": {},
   "source": [
    "## Step 2: Pre-Training Data Audit üõ°Ô∏è\n",
    "\n",
    "**Before training**, let's validate data quality and check for potential bias.\n",
    "\n",
    "### üéì Why Pre-Training Audits Matter\n",
    "\n",
    "**The \"Garbage In, Garbage Out\" Problem**: If your training data is biased, your model will learn and amplify that bias. Historical lending data often reflects:\n",
    "- **Historical discrimination**: E.g., women were denied loans more often in the 1990s\n",
    "\n",
    "- **Proxy variables**: Attributes like ZIP code or education can be proxies for protected attributes**Result**: If this audit fails, **stop and fix your data** before training. Training on bad data wastes compute and produces biased models.\n",
    "\n",
    "- **Class imbalance**: If 90% of approvals are male, the model may default to predicting \"approve\" for males\n",
    "\n",
    "- ‚úÖ **Class balance**: At least 20% representation of both classes (approved/rejected)\n",
    "\n",
    "**What this audit checks**:- ‚úÖ **Fairness baseline**: Demographic distribution (are protected groups represented?)\n",
    "\n",
    "- ‚úÖ **Data completeness**: No missing values in critical fields (target, protected attributes)- ‚úÖ **Data quality**: Valid ranges (e.g., age > 0, credit_amount > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c7f0ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ°Ô∏è  Running pre-training data audit...\n",
      "\n",
      "‚úÖ Data integrity assertions passed\n",
      "\n",
      "\n",
      "[Venturalitica] üõ°  Enforcing policy: ../../policies/loan/risks.oscal.yaml\n",
      "  Evaluating Control 'credit-data-imbalance': Data Quality: Minority class (rejected loans) shou...\n",
      "    [Binding] Virtual Role 'target' bound to Variable 'target' (Column: 'target')\n",
      "  Evaluating Control 'credit-data-bias': Pre-training Fairness: Disparate impact ratio shou...\n",
      "    [Binding] Virtual Role 'target' bound to Variable 'target' (Column: 'target')\n",
      "    [Binding] Virtual Role 'dimension' bound to Variable 'gender' (Column: 'gender')\n",
      "  Evaluating Control 'credit-age-disparate': Disparate impact ratio for raw age (Proxy for seni...\n",
      "    [Binding] Virtual Role 'target' bound to Variable 'target' (Column: 'target')\n",
      "    [Binding] Virtual Role 'dimension' bound to Variable 'age' (Column: 'age')\n",
      "  ‚ùå FAIL | Controls: 2/3 passed\n",
      "    ‚úì [credit-data-imbalance] Data Quality: Minority class (rejected l...: 0.429 (Limit: gt0.2)\n",
      "    ‚úì [credit-data-bias] Pre-training Fairness: Disparate impact ...: 0.897 (Limit: gt0.8)\n",
      "    ‚úó [credit-age-disparate] Disparate impact ratio for raw age (Prox...: 0.286 (Limit: gt0.5)\n",
      "  ‚úì Results cached for 'venturalitica push'\n",
      "\n",
      "üìä Pre-Training Audit Complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"üõ°Ô∏è  Running pre-training data audit...\\n\")\n",
    "\n",
    "# Data integrity checks\n",
    "assert 'gender' in df.columns, \"Critical: 'gender' column missing for fairness audit\"\n",
    "assert 'target' in df.columns, \"Critical: 'target' column missing\"\n",
    "assert df['target'].nunique() == 2, \"Critical: Target must be binary (0/1)\"\n",
    "print(\"‚úÖ Data integrity assertions passed\\n\")\n",
    "\n",
    "# Load policy and run audit\n",
    "policy_path = Path(\"../../policies/loan/risks.oscal.yaml\")\n",
    "results = vl.enforce(data=df, policy=str(policy_path))\n",
    "\n",
    "print(\"\\nüìä Pre-Training Audit Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e813840",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Training Data üîß\n",
    "\n",
    "Split data into train/test sets and prepare features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f96589a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train set: 800 samples\n",
      "‚úÖ Test set:  200 samples\n"
     ]
    }
   ],
   "source": [
    "# Split features and target\n",
    "# ‚ö†Ô∏è CRITICAL: Drop both 'target' and 'class' to avoid data leaking\n",
    "X = df.drop(columns=['target', 'class'])\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Train set: {len(X_train)} samples\")\n",
    "print(f\"‚úÖ Test set:  {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba86a6eb",
   "metadata": {},
   "source": [
    "## Step 4: Train Model with Governance Wrap ü§ñ\n",
    "\n",
    "### üéì What is \"Governance Wrapping\"?\n",
    "\n",
    "**The Challenge**: Most ML frameworks (sklearn, PyTorch, TensorFlow) know nothing about fairness. You train a model, it makes predictions, but there's no built-in way to check if those predictions are discriminatory.\n",
    "\n",
    "**The Solution**: `vl.wrap()` creates a **transparent governance layer** around your model:\n",
    "\n",
    "\n",
    "```pythonüí° **Pro tip**: In production, use `vl.wrap()` to monitor live traffic and trigger alerts when fairness metrics drift below thresholds.\n",
    "\n",
    "base_model = LogisticRegression()  # Your usual sklearn model\n",
    "\n",
    "model = vl.wrap(base_model, policy=\"fairness.yaml\")  # Wrapped version**Key insight**: The wrapper is **non-invasive**. It doesn't change your training code, model architecture, or predictions‚Äîit just adds an audit layer. Your existing sklearn pipelines work unchanged.\n",
    "\n",
    "model.fit(X_train, y_train)  # Train normally\n",
    "\n",
    "predictions = model.predict(X_test)  # Auto-audits on every predict!4. **Logs results**: Stores audit artifacts for compliance documentation\n",
    "\n",
    "```3. **Evaluates policy controls**: Computes demographic parity, equalized odds, calibration\n",
    "\n",
    "2. **Binds protected attributes**: Matches predictions to demographic data (gender, age, etc.)\n",
    "\n",
    "**What the wrapper does**:1. **Intercepts `.predict()` calls**: Before returning predictions, it runs fairness checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8cfe95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Training model with governance wrap...\n",
      "\n",
      "\n",
      "[Venturalitica] üõ°  Enforcing policy: ../../policies/loan/governance-baseline.oscal.yaml\n",
      "  Evaluating Control 'A.1': Automated check for Demographic Parity...\n",
      "    [Binding] Virtual Role 'target' bound to Variable 'target' (Column: 'target')\n",
      "    [Binding] Virtual Role 'prediction' bound to Variable 'prediction' (Column: 'prediction')\n",
      "    [Binding] Virtual Role 'dimension' bound to Variable 'gender' (Column: 'gender')\n",
      "  Evaluating Control 'A.2': Automated check for Classification Accuracy...\n",
      "    [Binding] Virtual Role 'target' bound to Variable 'target' (Column: 'target')\n",
      "    [Binding] Virtual Role 'prediction' bound to Variable 'prediction' (Column: 'prediction')\n",
      "  ‚ùå FAIL | Controls: 0/1 passed\n",
      "    ‚úó [A.2] Automated check for Classification Accur...: 0.000 (Limit: >=0.8)\n",
      "  ‚úì Results cached for 'venturalitica push'\n",
      "‚úÖ Model trained successfully!\n",
      "   Accuracy: 73.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"ü§ñ Training model with governance wrap...\\n\")\n",
    "\n",
    "# Build sklearn pipeline (standard feature engineering + model)\n",
    "numeric_features = X.select_dtypes(include=['number']).columns.tolist()\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Fill missing with median\n",
    "    ('scaler', StandardScaler())  # Normalize to mean=0, std=1\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', numeric_pipeline, numeric_features)],\n",
    "    remainder='drop'  # Drop non-numeric features (in a real scenario, encode categoricals)\n",
    ")\n",
    "\n",
    "base_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# üéØ WRAP THE MODEL WITH GOVERNANCE\n",
    "# This is the key step: we add fairness checks WITHOUT changing the model itself\n",
    "fairness_policy = Path(\"../../policies/loan/governance-baseline.oscal.yaml\")\n",
    "model = vl.wrap(base_pipeline, policy=str(fairness_policy))\n",
    "# ‚¨ÜÔ∏è Now 'model' is a GovernanceWrapper that behaves like base_pipeline but audits every predict()\n",
    "\n",
    "# Train as usual - the wrapper is transparent during training\n",
    "model.fit(X_train, y_train)\n",
    "accuracy = model.score(X_test, y_test)\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Model trained successfully!\")print(f\"\\nüîí Governance layer active: Every .predict() call will trigger fairness audits\")\n",
    "print(f\"   Accuracy: {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7037cf",
   "metadata": {},
   "source": [
    "**Governance wrap design notes**\n",
    "- `vl.wrap()` keeps the original feature names visible to the audit engine even though sklearn pipelines transform them.\n",
    "- Passing the fairness policy here means every `predict()` call will trigger post-training audits automatically.\n",
    "- Using `audit_data=test_df` ensures demographics are available during prediction-time checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b21c9",
   "metadata": {},
   "source": [
    "## Step 5: Post-Training Fairness Audit üõ°Ô∏è\n",
    "\n",
    "When we call `.predict()`, the governance wrapper **automatically triggers** fairness audits!\n",
    "\n",
    "### üéì What Fairness Metrics Are We Checking?\n",
    "\n",
    "**No code changes needed** - compliance is baked into the wrapper! üéâ\n",
    "\n",
    "The policy evaluates multiple fairness definitions (no single metric captures all discrimination):\n",
    "\n",
    "   - **Con**: Can conflict with demographic parity\n",
    "\n",
    "1. **Demographic Parity** (Statistical Parity)   - **Pro**: Ensures predictions mean the same thing for all groups\n",
    "\n",
    "   - $P(\\hat{y}=1 | \\text{male}) \\approx P(\\hat{y}=1 | \\text{female})$   - \"When the model predicts 'approve', the true approval rate should be the same across groups\"\n",
    "\n",
    "   - \"Approval rates should be similar across genders\"   - $P(y=1 | \\hat{y}=1, \\text{male}) \\approx P(y=1 | \\hat{y}=1, \\text{female})$\n",
    "\n",
    "   - **Pro**: Easy to explain to non-technical stakeholders3. **Calibration** (Predictive Parity)\n",
    "\n",
    "   - **Con**: Ignores base rates (what if one group has genuinely higher creditworthiness?)\n",
    "\n",
    "   - **Con**: Requires labeled test data\n",
    "\n",
    "2. **Equalized Odds** (Equal Opportunity)   - **Pro**: Accounts for ground truth labels\n",
    "\n",
    "   - $P(\\hat{y}=1 | y=1, \\text{male}) \\approx P(\\hat{y}=1 | y=1, \\text{female})$ (True Positive Rate parity)   - \"Error rates should be equal across groups\"\n",
    "   - $P(\\hat{y}=0 | y=0, \\text{male}) \\approx P(\\hat{y}=0 | y=0, \\text{female})$ (True Negative Rate parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34f826f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ°Ô∏è  Running post-training fairness audit...\n",
      "\n",
      "\n",
      "[Venturalitica] üõ°  Enforcing policy: ../../policies/loan/governance-baseline.oscal.yaml\n",
      "  Evaluating Control 'A.1': Automated check for Demographic Parity...\n",
      "    [Binding] Virtual Role 'target' bound to Variable 'target' (Column: 'target')\n",
      "    [Binding] Virtual Role 'prediction' bound to Variable 'prediction' (Column: 'prediction')\n",
      "    [Binding] Virtual Role 'dimension' bound to Variable 'gender' (Column: 'gender')\n",
      "  Evaluating Control 'A.2': Automated check for Classification Accuracy...\n",
      "    [Binding] Virtual Role 'target' bound to Variable 'target' (Column: 'target')\n",
      "    [Binding] Virtual Role 'prediction' bound to Variable 'prediction' (Column: 'prediction')\n",
      "  ‚ùå FAIL | Controls: 1/2 passed\n",
      "    ‚úì [A.1] Automated check for Demographic Parity...: 0.027 (Limit: <0.1)\n",
      "    ‚úó [A.2] Automated check for Classification Accur...: 0.730 (Limit: >=0.8)\n",
      "  ‚úì Results cached for 'venturalitica push'\n",
      "‚úÖ Predictions generated: 200 samples\n",
      "   Fairness audit completed automatically!\n"
     ]
    }
   ],
   "source": [
    "print(\"üõ°Ô∏è  Running post-training fairness audit...\\n\")\n",
    "\n",
    "# Predict on test set - this triggers automatic fairness audits!\n",
    "test_df = df.iloc[X_test.index].copy()\n",
    "predictions = model.predict(X_test, audit_data=test_df)\n",
    "\n",
    "print(f\"‚úÖ Predictions generated: {len(predictions)} samples\")\n",
    "print(f\"   Fairness audit completed automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be8401",
   "metadata": {},
   "source": [
    "## Step 6: Review Compliance Results üìä\n",
    "\n",
    "Let's examine the audit results in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edbadd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä DETAILED COMPLIANCE REPORT\n",
      "======================================================================\n",
      "\n",
      "‚úÖ PASSED | A.1\n",
      "   Value: 0.0267857142857143\n",
      "\n",
      "‚ùå FAILED | A.2\n",
      "   Value: 0.73\n",
      "\n",
      "======================================================================\n",
      "SUMMARY: 1/2 controls passed (50%)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "results = model.last_audit_results\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä DETAILED COMPLIANCE REPORT\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for r in results:\n",
    "    status = \"‚úÖ PASSED\" if r.passed else \"‚ùå FAILED\"\n",
    "    print(f\"{status} | {r.control_id}\")\n",
    "    print(f\"   Value: {r.actual_value}\")\n",
    "    \n",
    "    # Validation checks\n",
    "    assert r.actual_value is not None, f\"Critical: Control {r.control_id} returned null\"\n",
    "    \n",
    "    # Flag suspicious perfect metrics\n",
    "    if float(r.actual_value) in [0.0, 1.0]:\n",
    "        print(f\"   ‚ö†Ô∏è  SUSPICIOUS: Perfect value {r.actual_value}\")\n",
    "        if 'acc' in r.control_id.lower() or 'recall' in r.control_id.lower():\n",
    "            assert float(r.actual_value) > 0.0, \"Performance metric is 0.0 - check alignment!\"\n",
    "    print()\n",
    "\n",
    "# Summary\n",
    "passed = sum(1 for r in results if r.passed)\n",
    "total = len(results)\n",
    "print(\"=\"*70)\n",
    "print(f\"SUMMARY: {passed}/{total} controls passed ({passed/total*100:.0f}%)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24694746",
   "metadata": {},
   "source": [
    "**Interpreting results**\n",
    "- Look for failed controls to spot fairness or quality gaps quickly.\n",
    "- `actual_value` shows the measured metric; perfect 0/1 values can be suspicious, so we flag them for review.\n",
    "- Re-run after adjusting data or policy thresholds to see compliance improve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e050a2b",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You just built a **production-grade ML pipeline** with:\n",
    "- ‚úÖ Pre-training data quality audits\n",
    "- ‚úÖ Governance-wrapped model training\n",
    "- ‚úÖ Automatic post-training fairness validation\n",
    "- ‚úÖ OSCAL-native compliance reports\n",
    "\n",
    "### Model Performance\n",
    "- **Accuracy:** {accuracy:.1%}\n",
    "- **Compliance:** {passed}/{total} controls passed\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Option A: Experiment with Policies** üìù\n",
    "- Edit `policies/loan/governance-baseline.oscal.yaml`\n",
    "- Adjust fairness thresholds\n",
    "- Add custom controls\n",
    "\n",
    "**Option B: Try Different Models** üî¨\n",
    "- Replace LogisticRegression with RandomForest\n",
    "- Try XGBoost or LightGBM\n",
    "- Compare compliance scores\n",
    "\n",
    "**Option C: Add MLOps Integration** üöÄ\n",
    "- Open `02_mlops_integration.py`\n",
    "- Track experiments with MLflow\n",
    "- Version control your compliance reports\n",
    "\n",
    "**Option D: Production Deployment** üè≠\n",
    "- Open `03_production_ready.py`\n",
    "- See batch inference patterns\n",
    "- Learn about continuous compliance monitoring\n",
    "\n",
    "---\n",
    "\n",
    "**You're now ready to build compliant AI systems! üéì‚ú®**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98721cd",
   "metadata": {},
   "source": [
    "### How to interpret the metrics\n",
    "- **Accuracy**: share of test records predicted correctly. Use it to gauge basic model fit; pair it with class balance so high accuracy is not just predicting the majority class.\n",
    "- **Fairness audit (passed/total)**: number of policy controls that cleared across the fairness policy. `passed`/`total` gives quick compliance coverage; inspect failing controls to see which protected attribute or threshold was breached.\n",
    "- **Compliance messages**: each control‚Äôs message explains what was checked (e.g., parity gap, threshold, missing documentation) and why it passed or failed.\n",
    "- **Provenance**: `source` or policy path indicates which policy file defined the rule‚Äîuse it to trace back to governance requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venturalitica-integration (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
